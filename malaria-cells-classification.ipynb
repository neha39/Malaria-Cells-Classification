{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "e3d733161226253a066925d0c5b0270cd9dda6dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing Necessary Libraries.\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cell_images']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb  __output__.json\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48dabdc624053dcefbaf915c12ab1685b37deefb"
   },
   "source": [
    "## Data Preparation\n",
    "Data Preparation: We will make data and labels list where data will be image to array implementation which contains RGB values of each image and label will be class of cellls here I will be taking 0 and 1 for two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "55e4277c6dd27d5121f0d1e1c6a9c438c8b0178d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "Parasitized = os.listdir(\"../input/cell_images/cell_images/Parasitized/\")\n",
    "for a in Parasitized:\n",
    "    try:\n",
    "        image = cv2.imread(\"../input/cell_images/cell_images/Parasitized/\"+a)\n",
    "        image_from_array = Image.fromarray(image, 'RGB')\n",
    "        size_image = image_from_array.resize((50, 50))\n",
    "        data.append(np.array(size_image))\n",
    "        labels.append(0)\n",
    "    except AttributeError:\n",
    "        print(\"\")\n",
    "\n",
    "Uninfected = os.listdir(\"../input/cell_images/cell_images/Uninfected/\")\n",
    "for b in Uninfected:\n",
    "    try:\n",
    "        image = cv2.imread(\"../input/cell_images/cell_images/Uninfected/\"+b)\n",
    "        image_from_array = Image.fromarray(image, 'RGB')\n",
    "        size_image = image_from_array.resize((50, 50))\n",
    "        data.append(np.array(size_image))\n",
    "        labels.append(1)\n",
    "    except AttributeError:\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "20f0864cef135e3f0f855c2b282564daafce5e90"
   },
   "outputs": [],
   "source": [
    "Cells = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06279214b3d3a4cfb271735da24bfef300a941ce"
   },
   "source": [
    "Saving and Loading the data we prepared so next time we can load it from saved .npy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "a2811c424120bbe269f9732267b2dcd0cf3176f4"
   },
   "outputs": [],
   "source": [
    "np.save(\"Cells\",Cells)\n",
    "np.save(\"labels\",labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "091b656deae7ba5081d6172592ef2cf020b5a944"
   },
   "outputs": [],
   "source": [
    "Cells = np.load(\"Cells.npy\")\n",
    "labels = np.load(\"labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1bdefb7fad9b21ece7eef95c814a8cc1de007ee9"
   },
   "source": [
    "Do Train/Test Split of data and labels that prepared in early section. Classes are defined as the unique labels in the data. Here it will be 2 as Parasitized:0 and Uninfected:1, here 0 and 1 are the mapping in labels for these two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "892344c34074cb016e8f7629a90ecc4cb685d4a3"
   },
   "outputs": [],
   "source": [
    "s = np.arange(Cells.shape[0])\n",
    "np.random.shuffle(s)\n",
    "Cells = Cells[s]\n",
    "labels = labels[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "94170ee173195ac59d13fdb9eface34c4fe4ea89"
   },
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(labels))\n",
    "len_data = len(Cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "918d55e8b11fbd2ada0d2fc1af4fed8e38ad3511"
   },
   "outputs": [],
   "source": [
    "(x_train,x_test) = Cells[(int)(0.1*len_data):],Cells[:(int)(0.1*len_data)]\n",
    "x_train = x_train.astype('float32')/255 # As we are working on image data we are normalizing data by divinding 255.\n",
    "x_test = x_test.astype('float32')/255\n",
    "train_len = len(x_train)\n",
    "test_len = len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "0e6a7a58c9a334bf4dd98816b1e531b939141dab"
   },
   "outputs": [],
   "source": [
    "(y_train, y_test) = labels[(int)(0.1*len_data):], labels[:(int)(0.1*len_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c323ca5491ba2163b8a841e202e87fcd60d001c"
   },
   "source": [
    "\n",
    "## One hot encoding:\n",
    "Here the problem has two classes so last output layer of neural network will have 2 neurons one for each class, One hot encoding will help us to change labels in binary format. example: 2 can be represented as [1 0] if output layer has 2 neurons and [0 0 1 0] if output has 4 neurons/classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "395fe9f02d90ad22bb46533c8eb2ff9e71cfa4c5"
   },
   "outputs": [],
   "source": [
    "#Doing One hot encoding as classifier has multiple classes\n",
    "y_train = keras.utils.to_categorical(y_train,num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e38f9e1c11de8cf3e1a2d0d08fff512a9b5d233"
   },
   "source": [
    "\n",
    "## Create Sequential Model:\n",
    "Here I will be using Relu{max(0,z)}, You can try tanh/sigmoid/Leaky Relu for finding performance on various activation functions.Our output layer will be softmax activation rather than sigmoid as we have more than one class to classify. softmax activation calculates e^value/sum(all_values_in_axis[0 or 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "49b30be033cb0651e49bf1c67225ec96d6318bc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 50, 50, 16)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 25, 25, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 25, 25, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               1152500   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1002      \n",
      "=================================================================\n",
      "Total params: 1,164,046\n",
      "Trainable params: 1,164,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#creating sequential model\n",
    "model=Sequential()\n",
    "model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8e84a9b2a34fc4bb3fb5f23c1b5988eb30efbeb"
   },
   "source": [
    "compile the model with loss as categorical_crossentropy and using adam optimizer you can test result by trying RMSProp as well as Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "c3099daad97de53eb6c781817a2982e71e2712c7"
   },
   "outputs": [],
   "source": [
    "# compile the model with loss as categorical_crossentropy and using adam optimizer you can test result by trying RMSProp as well as Momentum\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f3ade9ea18c9a5550007dfd68e7f21b00f4e5558"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24803/24803 [==============================] - 33s 1ms/step - loss: 0.4792 - acc: 0.7555\n",
      "Epoch 2/20\n",
      "24803/24803 [==============================] - 32s 1ms/step - loss: 0.1793 - acc: 0.9415\n",
      "Epoch 3/20\n",
      "24803/24803 [==============================] - 32s 1ms/step - loss: 0.1522 - acc: 0.9518\n",
      "Epoch 4/20\n",
      "24803/24803 [==============================] - 32s 1ms/step - loss: 0.1362 - acc: 0.9561\n",
      "Epoch 5/20\n",
      "24803/24803 [==============================] - 31s 1ms/step - loss: 0.1295 - acc: 0.9573\n",
      "Epoch 6/20\n",
      "24803/24803 [==============================] - 33s 1ms/step - loss: 0.1193 - acc: 0.9604\n",
      "Epoch 7/20\n",
      "24803/24803 [==============================] - 32s 1ms/step - loss: 0.1099 - acc: 0.9616\n",
      "Epoch 8/20\n",
      "24803/24803 [==============================] - 32s 1ms/step - loss: 0.1039 - acc: 0.9638\n",
      "Epoch 9/20\n",
      "24803/24803 [==============================] - 32s 1ms/step - loss: 0.0925 - acc: 0.9675\n",
      "Epoch 10/20\n",
      "24803/24803 [==============================] - 32s 1ms/step - loss: 0.0878 - acc: 0.9688\n",
      "Epoch 11/20\n",
      "24803/24803 [==============================] - 32s 1ms/step - loss: 0.0772 - acc: 0.9727\n",
      "Epoch 12/20\n",
      "24803/24803 [==============================] - 32s 1ms/step - loss: 0.0717 - acc: 0.9738\n",
      "Epoch 13/20\n",
      "23300/24803 [===========================>..] - ETA: 1s - loss: 0.0599 - acc: 0.9787"
     ]
    }
   ],
   "source": [
    "#Fit the model with min batch size as 50[can tune batch size to some factor of 2^power ] \n",
    "model.fit(x_train, y_train, batch_size=50, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b6b1f3323b3ae26b8440c878d16c281d1c466c6"
   },
   "source": [
    "## Check the accuracy on Test data:¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "75a25753e1df3837390d18a0f87dea3f40d482ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2755/2755 [==============================] - 1s 528us/step\n",
      "\n",
      " Test_Accuracy:- 0.9600725952813067\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('\\n', 'Test_Accuracy:-', accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a32aa1fdf4b51e7e411c559bf6493e5f232aa15"
   },
   "source": [
    "## Save the model weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "1dfdaf7f9f29806819e3f42e5de393cca4e8be7a"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('cells.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "8ca8e043518cb3d46147dee35436f7dbfd32be23"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
